# -*- coding: utf-8 -*-
"""Credit_ml_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1URkRzdCW3V_h3yTJwJE8NxCvEjznowr1
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("corporateCreditRatingWithFinancialRatios.csv")
df.head()

print("Shape:", df.shape)
print("\nColumns:\n", df.columns.tolist())
print("\nMissing values:\n", df.isnull().sum())
df.info()

print(df['Rating'].value_counts())
print(df['Binary Rating'].value_counts())

# Step 1: LOCK FEATURE SET & SAVE SUBSET (run this in Colab)
import os, json
import pandas as pd

# --- adjust filename if different ---
fname = "corporateCreditRatingWithFinancialRatios.csv"

if not os.path.exists(fname):
    print("File not found in working dir. Files here:\n", os.listdir())
else:
    df = pd.read_csv(fname)
    print("Original shape:", df.shape)
    print("Columns available:", df.columns.tolist())

    # FINAL feature set (AV-computable + present in your dataset)
    feature_cols = [
        'Sector',
        'Current Ratio',
        'Long-term Debt / Capital',
        'Debt/Equity Ratio',
        'Gross Margin',
        'Operating Margin',
        'EBIT Margin',
        'Pre-Tax Profit Margin',
        'Net Profit Margin',
        'Asset Turnover',
        'ROE - Return On Equity',
        'ROA - Return On Assets',
        'Operating Cash Flow Per Share',
        'Free Cash Flow Per Share'
    ]

    # quick check for missing columns
    missing = [c for c in feature_cols if c not in df.columns]
    if missing:
        print("⚠️ The following required columns are MISSING from your file:\n", missing)
        print("Please confirm the exact column names or upload the correct CSV.")
    else:
        # keep also Rating and Rating Date for next steps
        keep_cols = feature_cols + ['Rating', 'Rating Date']
        df_sel = df[keep_cols].copy()

        # basic sanity
        print("\nSelected shape:", df_sel.shape)
        display(df_sel.head(6))
        print("\nDtypes:\n", df_sel.dtypes)

        # save artifacts for later steps
        with open("feature_list.json", "w") as f:
            json.dump(feature_cols, f, indent=2)
        df_sel.to_csv("df_features_selected.csv", index=False)
        print("\nSaved files: feature_list.json, df_features_selected.csv")

# Step 2: TARGET PREPARATION
import pandas as pd
import json

# load the subset from step 1
df_sel = pd.read_csv("df_features_selected.csv")

# create mapping dict: rating -> integer
unique_ratings = sorted(df_sel['Rating'].unique(), key=lambda x: x)
rating2id = {r: i for i, r in enumerate(unique_ratings)}
id2rating = {i: r for r, i in rating2id.items()}

# apply mapping
df_sel['Rating_Label'] = df_sel['Rating'].map(rating2id)

print("✅ Encoded target added as 'Rating_Label'")
print("\nRating → ID mapping:\n", rating2id)
print("\nSample:\n", df_sel[['Rating', 'Rating_Label']].head())

# check counts again
print("\nClass distribution:\n", df_sel['Rating_Label'].value_counts())

# save mapping for later use
with open("rating_mapping.json", "w") as f:
    json.dump({"rating2id": rating2id, "id2rating": id2rating}, f, indent=2)

df_sel.to_csv("df_features_ready.csv", index=False)
print("\nSaved: df_features_ready.csv and rating_mapping.json")

# ----- STEP: ONE-HOT ENCODING ONLY -----
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
import joblib

# Load dataset
df = pd.read_csv("df_features_ready.csv")

# Rare class drop
class_counts = df['Rating_Label'].value_counts()
rare = class_counts[class_counts < 2].index.tolist()
if rare:
    df = df[~df['Rating_Label'].isin(rare)].reset_index(drop=True)

# Features
feature_cols = [
    'Sector', 'Current Ratio', 'Long-term Debt / Capital',
    'Debt/Equity Ratio', 'Gross Margin', 'Operating Margin',
    'EBIT Margin', 'Pre-Tax Profit Margin', 'Net Profit Margin',
    'Asset Turnover', 'ROE - Return On Equity', 'ROA - Return On Assets',
    'Operating Cash Flow Per Share', 'Free Cash Flow Per Share'
]

X = df[feature_cols].copy()
y = df['Rating_Label'].copy()

# OneHotEncoder on Sector
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

X_sector = ohe.fit_transform(X[['Sector']])

# Convert OHE output to DataFrame
ohe_names = ohe.get_feature_names_out(['Sector'])
X_ohe_df = pd.DataFrame(X_sector, columns=ohe_names, index=X.index)

# Numeric part
X_num = X.drop(columns=['Sector']).reset_index(drop=True)

# Final encoded X
X_encoded = pd.concat([X_ohe_df.reset_index(drop=True), X_num], axis=1)

# Save
X_encoded.to_csv("X_encoded.csv", index=False)
y.to_csv("y_encoded.csv", index=False)
joblib.dump(ohe, "ohe_sector.joblib")

print("✅ Encoding complete.")
print("X_encoded shape:", X_encoded.shape)

import joblib

joblib.dump(ohe, "ohe_sector.joblib", compress=3)

ohe = joblib.load("ohe_sector.joblib")

import pickle

# Save
with open("ohe_sector.pkl", "wb") as f:
    pickle.dump(ohe, f)

# Load
with open("ohe_sector.pkl", "rb") as f:
    ohe = pickle.load(f)

df.columns

# Features (exclude target)
X = df.drop(columns=["Rating"])

# Target
y = df["Rating"]

import pandas as pd

# OneHot transform
sector_encoded = ohe.transform(df[['Sector']])
sector_encoded_df = pd.DataFrame(sector_encoded, columns=ohe.get_feature_names_out(['Sector']))

# Drop old Sector and concat new encoded
X = pd.concat([df.drop(columns=["Sector", "Rating"]), sector_encoded_df], axis=1)

X = pd.concat([df.drop(columns=["Sector", "Rating"]), sector_encoded_df], axis=1)

X = pd.concat([df.drop(columns=["Sector", "Rating"]), sector_encoded_df], axis=1)

# Drop Sector (already encoded), Rating (target), and Rating Date (string not useful for model)
X = pd.concat([df.drop(columns=["Sector", "Rating", "Rating Date"]), sector_encoded_df], axis=1)

# Target
y = df["Rating"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=200, random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from imblearn.over_sampling import SMOTE
import xgboost as xgb

# 1. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 2. Apply SMOTE on the training set
smote = SMOTE(random_state=42, k_neighbors=2)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("After SMOTE:", y_train_res.value_counts().head())

# 3. Encode labels
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train_res)   # must match X_train_res
y_test_enc  = le.transform(y_test)            # same encoder for test set

# 4. Train XGBoost classifier
xgb_clf = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    eval_metric="mlogloss",
    random_state=42
)

xgb_clf.fit(X_train_res, y_train_enc)

# 5. Evaluate
y_pred = xgb_clf.predict(X_test)

# Drop non-numeric columns (Sector is already encoded, Rating Date is useless)
X = pd.concat([df.drop(columns=["Sector", "Rating", "Rating Date"]), sector_encoded_df], axis=1)

# Features and target
y = df["Rating"]

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# SMOTE
smote = SMOTE(random_state=42, k_neighbors=2)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Encode target
le = LabelEncoder()
le.fit(y)
y_train_enc = le.transform(y_train_res)
y_test_enc = le.transform(y_test)

# Train XGBoost
xgb_clf = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    max_depth=5,
    n_estimators=500,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="mlogloss",
    random_state=42
)

xgb_clf.fit(X_train_res, y_train_enc)

# Evaluate
y_pred = xgb_clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test_enc, y_pred))
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))



print(len(X_train_res), len(y_train_res))
print(len(X_test), len(y_test))

from sklearn.model_selection import cross_val_score
import numpy as np

scores = cross_val_score(xgb_clf, X_encoded, le.transform(y), cv=5, scoring='accuracy')
print("5-fold CV Accuracy:", np.mean(scores), np.std(scores))

# Fixed Credit Rating Model - Addressing 100% Accuracy Issue

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import xgboost as xgb
import joblib

# Load your data
df = pd.read_csv("corporateCreditRatingWithFinancialRatios.csv")

print("Dataset shape:", df.shape)
print("Rating distribution:\n", df['Rating'].value_counts())

# 1. CAREFUL FEATURE SELECTION - Remove potential leakage sources
# These columns are GUARANTEED to cause leakage:
leakage_cols = [
    'Rating Date', 'Rating',        # Target and date
    'Corporation',                  # Company name = perfect predictor
    'CIK',                         # Company ID = perfect predictor
    'Ticker',                      # Stock ticker = perfect predictor
    'Binary Rating',               # Derived FROM rating = leakage!
    'Rating Agency'                # May have systematic biases
]

# Keep only legitimate predictive features
legitimate_features = [
    'SIC Code',                    # Industry code - legitimate
    'Sector',                      # Industry sector - legitimate
    'Current Ratio',               # Financial ratios - legitimate
    'Long-term Debt / Capital',
    'Debt/Equity Ratio',
    'Gross Margin',
    'Operating Margin',
    'EBIT Margin',
    'EBITDA Margin',
    'Pre-Tax Profit Margin',
    'Net Profit Margin',
    'Asset Turnover',
    'ROE - Return On Equity',
    'Return On Tangible Equity',
    'ROA - Return On Assets',
    'ROI - Return On Investment',
    'Operating Cash Flow Per Share',
    'Free Cash Flow Per Share'
]

# Verify all features exist in dataset
available_features = [col for col in legitimate_features if col in df.columns]
missing_features = [col for col in legitimate_features if col not in df.columns]

if missing_features:
    print(f"⚠️ Missing features: {missing_features}")

feature_cols = available_features

print("\nFeatures to use:")
for i, col in enumerate(feature_cols):
    print(f"{i+1}. {col}")

# 2. HANDLE MIXED DATA TYPES PROPERLY
X_temp = df[feature_cols].copy()

print("\nSelected features:")
for i, col in enumerate(feature_cols):
    print(f"{i+1}. {col}")

print(f"\nData types:")
print(X_temp.dtypes)

# Identify categorical and numeric columns
categorical_cols = X_temp.select_dtypes(include=['object', 'category']).columns.tolist()
numeric_cols = X_temp.select_dtypes(include=[np.number]).columns.tolist()

print(f"\nCategorical columns: {categorical_cols}")
print(f"Numeric columns: {len(numeric_cols)} total")

# Handle categorical variables
if categorical_cols:
    # OneHot encode all categorical columns
    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    cat_encoded = ohe.fit_transform(X_temp[categorical_cols])
    cat_encoded_df = pd.DataFrame(
        cat_encoded,
        columns=ohe.get_feature_names_out(categorical_cols),
        index=X_temp.index
    )

    # Combine with numeric features
    X_numeric = X_temp[numeric_cols]
    X_final = pd.concat([X_numeric, cat_encoded_df], axis=1)
else:
    X_final = X_temp[numeric_cols].copy()

print(f"\nFinal feature matrix shape: {X_final.shape}")

# Handle missing values (only for numeric columns now)
missing_before = X_final.isnull().sum().sum()
if missing_before > 0:
    print(f"Handling {missing_before} missing values...")
    # Fill numeric missing values with median
    numeric_features = X_final.select_dtypes(include=[np.number]).columns
    X_final[numeric_features] = X_final[numeric_features].fillna(X_final[numeric_features].median())

    # Fill any remaining missing values (from OHE) with 0
    X_final = X_final.fillna(0)

print(f"Missing values after cleaning: {X_final.isnull().sum().sum()}")

# 3. CHECK CORRELATIONS WITH TARGET
y = df['Rating']

# Encode target for correlation analysis
le_temp = LabelEncoder()
y_encoded = le_temp.fit_transform(y)

# Check correlations (only for numeric features)
numeric_cols = X_final.select_dtypes(include=[np.number]).columns
correlations = []

for col in numeric_cols:
    corr = np.corrcoef(X_final[col], y_encoded)[0, 1]
    correlations.append((col, abs(corr)))

correlations.sort(key=lambda x: x[1], reverse=True)

print("\nTop 10 feature correlations with target:")
for col, corr in correlations[:10]:
    print(f"{col}: {corr:.4f}")

# Flag suspiciously high correlations
suspicious_features = [col for col, corr in correlations if corr > 0.95]
if suspicious_features:
    print(f"\nWARNING: These features have suspiciously high correlation (>0.95):")
    for feat in suspicious_features:
        print(f"  - {feat}")
    print("Consider removing these features as they may cause leakage.")

# 4. PROPER TRAIN/TEST SPLIT WITH VALIDATION
X = X_final.copy()
y = df['Rating']

print(f"\nOriginal class distribution:")
class_counts = y.value_counts().sort_index()
print(class_counts)

# Remove classes with too few samples for stratified split (need at least 2)
min_samples_per_class = 2
rare_classes = class_counts[class_counts < min_samples_per_class].index.tolist()

if rare_classes:
    print(f"\nRemoving rare classes with <{min_samples_per_class} samples:")
    for cls in rare_classes:
        print(f"  - {cls}: {class_counts[cls]} samples")

    # Filter out rare classes
    mask = ~y.isin(rare_classes)
    X = X[mask].reset_index(drop=True)
    y = y[mask].reset_index(drop=True)

    print(f"\nAfter removing rare classes:")
    print(f"Dataset size: {len(X)} samples")
    print(f"Number of classes: {y.nunique()}")

    updated_counts = y.value_counts().sort_index()
    print("\nUpdated class distribution:")
    print(updated_counts)

# First split: Train (80%) and Test (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print(f"\nDataset splits:")
print(f"Training set: {len(X_train)} samples")
print(f"Test set: {len(X_test)} samples")

# Check class distribution
print(f"\nTraining set class distribution:")
print(y_train.value_counts())

# 5. APPLY SMOTE ONLY ON TRAINING DATA (with safety check)
min_neighbors = min(y_train.value_counts()) - 1
k_neighbors = max(1, min(2, min_neighbors))  # Ensure k >= 1

print(f"Using k_neighbors={k_neighbors} for SMOTE")

smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print(f"\nAfter SMOTE - Training set: {len(X_train_res)} samples")
print(f"Class distribution after SMOTE:\n{pd.Series(y_train_res).value_counts()}")

# 6. ENCODE LABELS
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train_res)
y_test_enc = le.transform(y_test)

# 7. START WITH SIMPLER MODEL
print("\n" + "="*50)
print("Training Random Forest (Baseline)")
print("="*50)

rf_model = RandomForestClassifier(
    n_estimators=100,  # Reduced complexity
    max_depth=10,      # Limit depth to prevent overfitting
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

rf_model.fit(X_train_res, y_train_res)  # Use original labels for RF

# Evaluate Random Forest
rf_pred = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)

print(f"Random Forest Test Accuracy: {rf_accuracy:.4f}")
if rf_accuracy > 0.99:
    print("⚠️  WARNING: Still getting >99% accuracy - likely data leakage!")

# 8. XGBoost WITH REDUCED COMPLEXITY
print("\n" + "="*50)
print("Training XGBoost (Reduced Complexity)")
print("="*50)

xgb_clf = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    max_depth=3,           # Much smaller depth
    n_estimators=50,       # Fewer trees
    learning_rate=0.1,     # Higher learning rate with fewer estimators
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=1,           # L1 regularization
    reg_lambda=1,          # L2 regularization
    eval_metric="mlogloss",
    random_state=42
)

xgb_clf.fit(X_train_res, y_train_enc)

# Evaluate XGBoost
xgb_pred = xgb_clf.predict(X_test)
xgb_pred_labels = le.inverse_transform(xgb_pred)
xgb_accuracy = accuracy_score(y_test, xgb_pred_labels)

print(f"XGBoost Test Accuracy: {xgb_accuracy:.4f}")
if xgb_accuracy > 0.99:
    print("⚠️  WARNING: Still getting >99% accuracy - likely data leakage!")

# 9. CROSS VALIDATION (PROPER WAY)
print("\n" + "="*50)
print("Cross Validation (Without SMOTE)")
print("="*50)

# Use original training data for CV (without SMOTE)
cv_scores = cross_val_score(
    RandomForestClassifier(n_estimators=50, max_depth=8, random_state=42),
    X_train, y_train,  # Original training data
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='accuracy'
)

print(f"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# 10. FEATURE IMPORTANCE ANALYSIS
print("\n" + "="*50)
print("Feature Importance Analysis")
print("="*50)

feature_importance = pd.DataFrame({
    'feature': X_final.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("Top 10 most important features:")
print(feature_importance.head(10))

# Check if any single feature dominates
top_feature_importance = feature_importance.iloc[0]['importance']
if top_feature_importance > 0.8:
    print(f"\n⚠️  WARNING: Top feature '{feature_importance.iloc[0]['feature']}' has {top_feature_importance:.1%} importance!")
    print("This suggests possible data leakage.")

# 11. FINAL RECOMMENDATIONS
print("\n" + "="*50)
print("RECOMMENDATIONS TO FIX 100% ACCURACY")
print("="*50)

recommendations = [
    "1. Remove 'Rating Date' completely - it likely contains leakage",
    "2. Check if any financial ratios are calculated using post-rating information",
    "3. Verify that all features represent information available BEFORE rating decision",
    "4. Consider using only fundamental financial metrics (Current Ratio, Debt/Equity, etc.)",
    "5. Try training on a random subset of features to identify leakage sources",
    "6. Use simpler models first (Logistic Regression) to establish baseline",
    "7. Implement proper time-based splits if you have temporal data"
]

for rec in recommendations:
    print(rec)

print(f"\nCurrent test accuracy: {max(rf_accuracy, xgb_accuracy):.4f}")
if max(rf_accuracy, xgb_accuracy) < 0.95:
    print("✅ Accuracy looks more realistic now!")
else:
    print("⚠️ Still need to investigate data leakage sources.")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE # Corrected import
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import joblib
import logging
import os

# Setup logging
logging.basicConfig(filename='model_training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load data
try:
    df = pd.read_csv("corporateCreditRatingWithFinancialRatios.csv")
    logging.info("Dataset loaded successfully. Shape: %s", df.shape)
except FileNotFoundError:
    logging.error("Input CSV not found.")
    raise

# Define features
feature_cols = [
    'Sector', 'Current Ratio', 'Long-term Debt / Capital', 'Debt/Equity Ratio',
    'Gross Margin', 'Operating Margin', 'EBIT Margin', 'Pre-Tax Profit Margin',
    'Net Profit Margin', 'Asset Turnover', 'ROE - Return On Equity',
    'ROA - Return On Assets', 'Operating Cash Flow Per Share', 'Free Cash Flow Per Share'
]

# Validate columns
missing = [c for c in feature_cols + ['Rating'] if c not in df.columns]
if missing:
    logging.error("Required columns missing: %s", missing)
    raise ValueError(f"Required columns missing: {missing}")

# Features and target
X = df[feature_cols].copy()
y = df['Rating'].copy()

# Group rare classes (fewer than 10 instances) into 'Other'
class_counts = y.value_counts()
logging.info("Original class distribution:\n%s", class_counts)
min_samples = 10
y = y.apply(lambda x: 'Other' if class_counts[x] < min_samples else x)
logging.info("Class distribution after grouping rare classes:\n%s", y.value_counts())

# Handle remaining rare classes (fewer than 2 instances for stratification)
class_counts = y.value_counts()
rare_classes = class_counts[class_counts < 2].index.tolist()
if rare_classes:
    logging.warning("Removing rare classes with fewer than 2 instances: %s", rare_classes)
    print(f"Removing rare classes: {rare_classes}")
    X = X[~y.isin(rare_classes)].reset_index(drop=True)
    y = y[~y.isin(rare_classes)].reset_index(drop=True)
    logging.info("Dataset shape after removing rare classes: %s", X.shape)

# Verify class counts
if y.value_counts().min() < 2:
    logging.error("After filtering, some classes still have fewer than 2 instances.")
    raise ValueError("After filtering, some classes still have fewer than 2 instances.")


# Train-test split
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    logging.info("Train-test split completed. Train shape: %s, Test shape: %s", X_train.shape, X_test.shape)
except ValueError as e:
    logging.error("Train-test split failed: %s", str(e))
    raise

# Encode target
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)
logging.info("Target classes: %s", list(le.classes_))

# Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['Sector']),
        ('num', 'passthrough', [col for col in feature_cols if col != 'Sector'])
    ])

# Pipeline with SMOTE
min_class_size = pd.Series(y_train_enc).value_counts().min()
k_neighbors = min(2, max(1, min_class_size - 1))  # Adjust k_neighbors dynamically
pipeline = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42, k_neighbors=k_neighbors)),
    ('classifier', xgb.XGBClassifier(
        objective="multi:softmax",
        num_class=len(le.classes_),
        eval_metric="mlogloss",
        random_state=42
    ))
])

# Hyperparameter tuning (optional, uncomment to use)

param_grid = {
    'classifier__max_depth': [3, 5],
    'classifier__learning_rate': [0.05, 0.1],
    'classifier__n_estimators': [100, 300]
}
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)
grid_search.fit(X_train, y_train_enc)
pipeline = grid_search.best_estimator_
logging.info("Best parameters: %s", grid_search.best_params_)


# Fit pipeline
try:
    pipeline.fit(X_train, y_train_enc)
    logging.info("Model training completed.")
except Exception as e:
    logging.error("Model training failed: %s", str(e))
    raise

# Evaluate
y_pred = pipeline.predict(X_test)
print("Accuracy:", pipeline.score(X_test, y_test_enc))
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))
logging.info("Test accuracy: %s", pipeline.score(X_test, y_test_enc))
logging.info("Test F1-macro: %s", f1_score(y_test_enc, y_pred, average='macro'))


# Cross-validation
try:
    scores = cross_val_score(pipeline, X, le.transform(y), cv=5, scoring='f1_macro')
    print(f"5-fold CV F1-macro: {np.mean(scores):.4f} ± {np.std(scores):.4f}")
    logging.info("5-fold CV F1-macro: %s ± %s", np.mean(scores), np.std(scores))
except Exception as e:
    logging.error("Cross-validation failed: %s", str(e))
    raise

# Save artifacts
try:
    joblib.dump(pipeline, "credit_model_pipeline.joblib")
    joblib.dump(le, "label_encoder.joblib")
    logging.info("Model and encoder saved.")
    print("Model and encoder saved to disk.")
    if os.path.exists("credit_model_pipeline.joblib") and os.path.exists("label_encoder.joblib"):
        print("Files verified: credit_model_pipeline.joblib, label_encoder.joblib")
    else:
        logging.error("Saved files not found.")
        print("Error: Saved files not found.")
except Exception as e:
    logging.error("Failed to save model/encoder: %s", str(e))
    raise

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import classification_report, balanced_accuracy_score, accuracy_score
import xgboost as xgb
import joblib

df = pd.read_csv("corporateCreditRatingWithFinancialRatios.csv")

feature_cols = [
    'Sector', 'Current Ratio', 'Long-term Debt / Capital', 'Debt/Equity Ratio',
    'Gross Margin', 'Operating Margin', 'EBIT Margin', 'Pre-Tax Profit Margin',
    'Net Profit Margin', 'Asset Turnover', 'ROE - Return On Equity',
    'ROA - Return On Assets', 'Operating Cash Flow Per Share', 'Free Cash Flow Per Share'
]

X = df[feature_cols].copy()
y = df['Rating'].copy()

def collapse_rating(r):
    if r in ['AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-']:
        return 'High'
    elif r in ['BBB+', 'BBB', 'BBB-']:
        return 'Medium'
    else:
        return 'Low'

y_grouped = y.apply(collapse_rating)

print("Class distribution after grouping:\n", y_grouped.value_counts())

X_train, X_test, y_train, y_test = train_test_split(
    X, y_grouped, test_size=0.2, stratify=y_grouped, random_state=42
)

# ===============================
# 4. Encode Target
# ===============================
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['Sector']),
        ('num', 'passthrough', [col for col in feature_cols if col != 'Sector'])
    ])

xgb_clf = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    eval_metric="mlogloss",
    random_state=42
)

pipeline = ImbPipeline([
    ('preprocessor', preprocessor),
    ('classifier', xgb_clf)
])

# Compute class weights
sample_weights = compute_sample_weight("balanced", y_train_enc)

# ===============================
# 7. Train Model
# ===============================
pipeline.fit(X_train, y_train_enc, classifier__sample_weight=sample_weights)

# ===============================
# 8. Evaluate
# ===============================
y_pred = pipeline.predict(X_test)

print("Accuracy:", accuracy_score(y_test_enc, y_pred))
print("Balanced Accuracy:", balanced_accuracy_score(y_test_enc, y_pred))
print("\nClassification Report:\n", classification_report(y_test_enc, y_pred, target_names=le.classes_))

# ===============================
# 9. Save Artifacts
# ===============================
joblib.dump(pipeline, "credit_model_pipeline_grouped.joblib")
joblib.dump(le, "label_encoder_grouped.joblib")
print("Model + Label Encoder saved.")

"""other model

"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, balanced_accuracy_score, accuracy_score
from sklearn.utils.class_weight import compute_sample_weight
import xgboost as xgb
import joblib

df = pd.read_csv("corporateCreditRatingWithFinancialRatios.csv")

feature_cols = [
    'Sector', 'Current Ratio', 'Long-term Debt / Capital', 'Debt/Equity Ratio',
    'Gross Margin', 'Operating Margin', 'EBIT Margin', 'Pre-Tax Profit Margin',
    'Net Profit Margin', 'Asset Turnover', 'ROE - Return On Equity',
    'ROA - Return On Assets', 'Operating Cash Flow Per Share', 'Free Cash Flow Per Share'
]

X = df[feature_cols].copy()
y = df['Rating'].copy()

def collapse_rating(r):
    if r in ['AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-']:
        return 'High'
    elif r in ['BBB+', 'BBB', 'BBB-']:
        return 'Medium'
    else:
        return 'Low'

y_grouped = y.apply(collapse_rating)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y_grouped, test_size=0.2, stratify=y_grouped, random_state=42
)

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

# Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['Sector']),
        ('num', 'passthrough', [col for col in feature_cols if col != 'Sector'])
    ]
)

xgb_clf = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    eval_metric="mlogloss",
    random_state=42,
    n_jobs=-1
)

param_dist = {
    'max_depth': [3, 4, 5, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 400, 600],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.5, 1],
    'reg_lambda': [1, 2, 5]
}

# RandomizedSearch (faster than GridSearch)
search = RandomizedSearchCV(
    xgb_clf,
    param_distributions=param_dist,
    n_iter=30,  # number of random combinations
    cv=3,
    scoring='f1_macro',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Apply preprocessing manually before tuning
X_train_proc = preprocessor.fit_transform(X_train)
X_test_proc = preprocessor.transform(X_test)

# Class weights
sample_weights = compute_sample_weight("balanced", y_train_enc)

# Fit search
search.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)

print("Best Params:", search.best_params_)
best_model = search.best_estimator_

y_pred = best_model.predict(X_test_proc)

print("Accuracy:", accuracy_score(y_test_enc, y_pred))
print("Balanced Accuracy:", balanced_accuracy_score(y_test_enc, y_pred))
print("\nClassification Report:\n", classification_report(y_test_enc, y_pred, target_names=le.classes_))

# Save
joblib.dump(best_model, "credit_model_optimized.joblib")
joblib.dump(le, "label_encoder_grouped.joblib")
joblib.dump(preprocessor, "preprocessor.joblib")
print("Optimized model saved.")

from sklearn.ensemble import VotingClassifier, RandomForestClassifier
import lightgbm as lgb

# Define base models
rf = RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42)
lgbm = lgb.LGBMClassifier(objective="multiclass", class_weight="balanced", random_state=42)
xgbm = search.best_estimator_   # your optimized XGBoost

# Voting ensemble
ensemble = VotingClassifier(
    estimators=[('rf', rf), ('lgbm', lgbm), ('xgbm', xgbm)],
    voting='soft'  # soft = average probabilities
)

# Fit ensemble
ensemble.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)

# Evaluate
y_pred = ensemble.predict(X_test_proc)

print("Ensemble Accuracy:", accuracy_score(y_test_enc, y_pred))
print("Balanced Accuracy:", balanced_accuracy_score(y_test_enc, y_pred))
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))

"""other model

"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score
from sklearn.utils.class_weight import compute_sample_weight

from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier

import joblib

df = pd.read_csv("corporateCreditRatingWithFinancialRatios.csv")

feature_cols = [
    'Sector', 'Current Ratio', 'Long-term Debt / Capital', 'Debt/Equity Ratio',
    'Gross Margin', 'Operating Margin', 'EBIT Margin', 'Pre-Tax Profit Margin',
    'Net Profit Margin', 'Asset Turnover', 'ROE - Return On Equity',
    'ROA - Return On Assets', 'Operating Cash Flow Per Share', 'Free Cash Flow Per Share'
]

X = df[feature_cols].copy()
y = df['Rating'].copy()

# Collapse Ratings into 3 groups
def collapse_rating(r):
    if r in ['AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-']:
        return 'High'
    elif r in ['BBB+', 'BBB', 'BBB-']:
        return 'Medium'
    else:
        return 'Low'

y_grouped = y.apply(collapse_rating)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y_grouped, test_size=0.2, stratify=y_grouped, random_state=42
)

# Encode target
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

# Preprocessor (OHE + numeric passthrough)
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['Sector']),
        ('num', 'passthrough', [col for col in feature_cols if col != 'Sector'])
    ]
)

X_train_proc = preprocessor.fit_transform(X_train)
X_test_proc = preprocessor.transform(X_test)

set_config(enable_metadata_routing=True)

stacked_clf.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)

# 2. Base Models
# ===============================
rf = RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42)

lgbm = lgb.LGBMClassifier(
    objective="multiclass",
    class_weight="balanced",
    random_state=42
)

xgbm = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    eval_metric="mlogloss",
    random_state=42,
    n_estimators=300,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8
)

# ===============================
# 3. Stacking Ensemble
# ===============================
stacked_clf = StackingClassifier(
    estimators=[('rf', rf), ('lgbm', lgbm), ('xgbm', xgbm)],
    final_estimator=LogisticRegression(max_iter=200, class_weight="balanced"),
    cv=5,
    n_jobs=-1
)

# Fit with class weights
sample_weights = compute_sample_weight("balanced", y_train_enc)
stacked_clf.fit(X_train_proc, y_train_enc, rf__sample_weight=sample_weights)

# ===============================
# 4. Evaluate
# ===============================
y_pred = stacked_clf.predict(X_test_proc)

print("Stacked Ensemble Accuracy:", accuracy_score(y_test_enc, y_pred))
print("Stacked Balanced Accuracy:", balanced_accuracy_score(y_test_enc, y_pred))
print("\nClassification Report:\n", classification_report(y_test_enc, y_pred, target_names=le.classes_))

# ===============================
# 5. Save Artifacts
# ===============================
joblib.dump(stacked_clf, "credit_model_stacked.joblib")
joblib.dump(le, "label_encoder_grouped.joblib")
joblib.dump(preprocessor, "preprocessor.joblib")
print("Stacked model saved.")

from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
import xgboost as xgb
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report

# ===============================
# 1. Base Models (define fresh ones)
# ===============================
rf = RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42)

lgbm = lgb.LGBMClassifier(
    objective="multiclass",
    class_weight="balanced",
    random_state=42
)

xgbm = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    eval_metric="mlogloss",
    random_state=42,
    n_estimators=300,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8
)

# ===============================
# 2. Compute class weights
# ===============================
sample_weights = compute_sample_weight("balanced", y_train_enc)

# Train each base model with sample weights
rf.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)
lgbm.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)
xgbm.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)

# ===============================
# 3. Stacking Classifier
# ===============================
stacked_clf = StackingClassifier(
    estimators=[('rf', rf), ('lgbm', lgbm), ('xgbm', xgbm)],
    final_estimator=LogisticRegression(max_iter=200, class_weight="balanced"),
    cv=5,
    n_jobs=-1
)

# Fit stacked classifier (no sample_weight needed now)
stacked_clf.fit(X_train_proc, y_train_enc)

# ===============================
# 4. Evaluate
# ===============================
y_pred = stacked_clf.predict(X_test_proc)

print("Stacked Ensemble Accuracy:", accuracy_score(y_test_enc, y_pred))
print("Stacked Balanced Accuracy:", balanced_accuracy_score(y_test_enc, y_pred))
print("\nClassification Report:\n", classification_report(y_test_enc, y_pred, target_names=le.classes_))

import pandas as pd
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report
from sklearn.ensemble import VotingClassifier

# ===============================
# 1. Define Base Models Again
# ===============================
rf = RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42)

lgbm = lgb.LGBMClassifier(
    objective="multiclass",
    class_weight="balanced",
    random_state=42
)

xgbm = xgb.XGBClassifier(
    objective="multi:softmax",
    num_class=len(le.classes_),
    eval_metric="mlogloss",
    random_state=42,
    n_estimators=300,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8
)

# ===============================
# 2. Train Each Base Model
# ===============================
sample_weights = compute_sample_weight("balanced", y_train_enc)

rf.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)
lgbm.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)
xgbm.fit(X_train_proc, y_train_enc, sample_weight=sample_weights)

# Voting Ensemble
voting = VotingClassifier(
    estimators=[('rf', rf), ('lgbm', lgbm), ('xgbm', xgbm)],
    voting='soft'
)
voting.fit(X_train_proc, y_train_enc)

# Stacking Ensemble
stacked = StackingClassifier(
    estimators=[('rf', rf), ('lgbm', lgbm), ('xgbm', xgbm)],
    final_estimator=LogisticRegression(max_iter=200, class_weight="balanced"),
    cv=5,
    n_jobs=-1
)
stacked.fit(X_train_proc, y_train_enc)

# ===============================
# 3. Evaluate All Models
# ===============================
models = {
    "RandomForest": rf,
    "LightGBM": lgbm,
    "XGBoost": xgbm,
    "VotingEnsemble": voting,
    "StackedEnsemble": stacked
}

results = []
for name, model in models.items():
    y_pred = model.predict(X_test_proc)
    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test_enc, y_pred),
        "Balanced Accuracy": balanced_accuracy_score(y_test_enc, y_pred),
        "F1-macro": f1_score(y_test_enc, y_pred, average='macro')
    })

# Convert to DataFrame for clear comparison
results_df = pd.DataFrame(results)
print(results_df)

"""Final Model

"""

import joblib

# Save final model artifacts
joblib.dump(rf, "credit_rf_model.joblib")              # Best model
joblib.dump(preprocessor, "preprocessor.joblib")       # OHE + numeric passthrough
joblib.dump(le, "label_encoder.joblib")                # High/Medium/Low encoder

print("✅ Model, Preprocessor, and Label Encoder saved.")

# Load artifacts
rf_model = joblib.load("credit_rf_model.joblib")
preprocessor = joblib.load("preprocessor.joblib")
le = joblib.load("label_encoder.joblib")

import pandas as pd

def predict_credit_rating(input_data: dict):
    """
    input_data: dictionary with company financial ratios
    Example:
    {
        'Sector': 'Technology',
        'Current Ratio': 1.8,
        'Long-term Debt / Capital': 0.35,
        'Debt/Equity Ratio': 0.5,
        'Gross Margin': 0.45,
        'Operating Margin': 0.18,
        'EBIT Margin': 0.17,
        'Pre-Tax Profit Margin': 0.15,
        'Net Profit Margin': 0.12,
        'Asset Turnover': 0.8,
        'ROE - Return On Equity': 0.14,
        'ROA - Return On Assets': 0.09,
        'Operating Cash Flow Per Share': 2.5,
        'Free Cash Flow Per Share': 1.8
    }
    """
    # Convert dict → DataFrame
    df_input = pd.DataFrame([input_data])

    # Preprocess features
    X_proc = preprocessor.transform(df_input)

    # Predict
    pred_enc = rf_model.predict(X_proc)
    pred_label = le.inverse_transform(pred_enc)

    return pred_label[0]

sample_company = {
    'Sector': 'Technology',
    'Current Ratio': 1.8,
    'Long-term Debt / Capital': 0.35,
    'Debt/Equity Ratio': 0.5,
    'Gross Margin': 0.45,
    'Operating Margin': 0.18,
    'EBIT Margin': 0.17,
    'Pre-Tax Profit Margin': 0.15,
    'Net Profit Margin': 0.12,
    'Asset Turnover': 0.8,
    'ROE - Return On Equity': 0.14,
    'ROA - Return On Assets': 0.09,
    'Operating Cash Flow Per Share': 2.5,
    'Free Cash Flow Per Share': 1.8
}

print("Predicted Credit Rating Group:", predict_credit_rating(sample_company))

"""from alphavantage"""



from alpha_vantage.fundamentaldata import FundamentalData
import pandas as pd
import joblib

API_KEY = "WY9FJNSAA8LNT50H"
fd = FundamentalData(key=API_KEY, output_format="pandas")

# Balance sheet, income statement, cashflow
bs, _ = fd.get_balance_sheet_annual("AAPL")
is_, _ = fd.get_income_statement_annual("AAPL")
cf, _ = fd.get_cash_flow_annual("AAPL")

print(bs.head())

# 1. Safe converter
def safe_float(x):
    try:
        if x in [None, "None", "NaN", "nan", ""]:
            return 0.0
        return float(x)
    except Exception:
        return 0.0

# 2. Ratio computation
def compute_ratios_from_av(bs, is_, cf, sector="Technology"):
    bs0, is0, cf0 = bs.iloc[0], is_.iloc[0], cf.iloc[0]

    # Balance sheet
    total_assets = safe_float(bs0.get("totalAssets"))
    total_liab = safe_float(bs0.get("totalLiabilities"))
    current_assets = safe_float(bs0.get("totalCurrentAssets"))
    current_liab = safe_float(bs0.get("totalCurrentLiabilities"))
    long_term_debt = safe_float(bs0.get("longTermDebtNoncurrent"))
    total_equity = safe_float(bs0.get("totalShareholderEquity"))
    shares_out = safe_float(bs0.get("commonStockSharesOutstanding"))

    # Income statement
    revenue = safe_float(is0.get("totalRevenue"))
    gross_profit = safe_float(is0.get("grossProfit"))
    ebit = safe_float(is0.get("ebit"))
    net_income = safe_float(is0.get("netIncome"))
    pretax_income = safe_float(is0.get("incomeBeforeTax"))
    operating_income = safe_float(is0.get("operatingIncome"))

    # Cash flow
    op_cf = safe_float(cf0.get("operatingCashflow"))
    capex = safe_float(cf0.get("capitalExpenditures"))
    free_cf = op_cf + capex  # capex is negative, so this is effectively OCF - CapEx

    return {
        "Sector": sector,
        "Current Ratio": current_assets / current_liab if current_liab else 0,
        "Long-term Debt / Capital": long_term_debt / (long_term_debt + total_equity) if (long_term_debt + total_equity) else 0,
        "Debt/Equity Ratio": total_liab / total_equity if total_equity else 0,
        "Gross Margin": gross_profit / revenue if revenue else 0,
        "Operating Margin": operating_income / revenue if revenue else 0,
        "EBIT Margin": ebit / revenue if revenue else 0,
        "Pre-Tax Profit Margin": pretax_income / revenue if revenue else 0,
        "Net Profit Margin": net_income / revenue if revenue else 0,
        "Asset Turnover": revenue / total_assets if total_assets else 0,
        "ROE - Return On Equity": net_income / total_equity if total_equity else 0,
        "ROA - Return On Assets": net_income / total_assets if total_assets else 0,
        "Operating Cash Flow Per Share": op_cf / shares_out if shares_out else 0,
        "Free Cash Flow Per Share": free_cf / shares_out if shares_out else 0
    }

# 3. Prediction function
def predict_from_alphavantage(ticker, sector="Technology"):
    bs, _ = fd.get_balance_sheet_annual(ticker)
    is_, _ = fd.get_income_statement_annual(ticker)
    cf, _ = fd.get_cash_flow_annual(ticker)

    ratios = compute_ratios_from_av(bs, is_, cf, sector)
    df_input = pd.DataFrame([ratios])

    X_proc = preprocessor.transform(df_input)
    pred_enc = rf_model.predict(X_proc)
    pred_label = le.inverse_transform(pred_enc)

    return pred_label[0], ratios

pred, ratios = predict_from_alphavantage("AAPL", "Technology")
print("Ratios:", ratios)
print("Predicted Credit Rating Group:", pred)

def predict_batch(tickers, sector="Technology"):
    results = []

    for ticker in tickers:
        try:
            # Fetch statements
            bs, _ = fd.get_balance_sheet_annual(ticker)
            is_, _ = fd.get_income_statement_annual(ticker)
            cf, _ = fd.get_cash_flow_annual(ticker)

            # Compute ratios
            ratios = compute_ratios_from_av(bs, is_, cf, sector)
            df_input = pd.DataFrame([ratios])

            # Preprocess + predict
            X_proc = preprocessor.transform(df_input)
            pred_enc = rf_model.predict(X_proc)
            pred_label = le.inverse_transform(pred_enc)[0]

            # Store
            ratios["Ticker"] = ticker
            ratios["Predicted Rating"] = pred_label
            results.append(ratios)

        except Exception as e:
            print(f"❌ Failed for {ticker}: {e}")
            results.append({"Ticker": ticker, "Error": str(e)})

    return pd.DataFrame(results)

tickers = ["AAPL", "MSFT", "TSLA", "GOOGL"]
df_preds = predict_batch(tickers, sector="Technology")
print(df_preds[["Ticker", "Predicted Rating", "Current Ratio", "Debt/Equity Ratio"]])

import joblib

# Save final model artifacts
joblib.dump(rf, "../models/credit_rf_model.joblib")
joblib.dump(preprocessor, "../models/preprocessor.joblib")
joblib.dump(le, "../models/label_encoder.joblib")

print("✅ Model, Preprocessor, and Label Encoder saved in /models")
